fine tune the meta vqvae:
music_vqvae_train.py + meta_vqvae.yaml


add beat transformer to wavenet, use preprocessed music features
diffusion_wavenet_bt_train.py + diffusion_wavenet_bt.yaml + mmldm.models.diffusion.diffusion_wavenet_bt +
mmldm.models.denoise_wavenet_bt + diffusion_wavenet_bt_dataset + diffusion_wavenet_bt_logger


improve the diff wavenet with meta vqvae, by using the extracted feature and 48000 sr for CLAP
diffusion_wavenet_bt_train.py + diff_wavenet_with_metavqvae_v2.yaml + mmldm.models.diffusion.diffusion_wavenet_v2 +
diffusion_wavenet_v2_inference.py


try unet_1d as generation network
diffusion_wavenet_bt_train.py + diffusion_unet.yaml + mmldm.models.unet_1d +
diffusion_wavenet_v2_inference.py


change the condition methods in wavenet and add some activations. some improvements on the original wavenet
diffusion_wavenet_bt_train.py + diff_wavenet_with_metavqvae_v3.yaml


Use the CLAP embedding without pooling, modified from v3.
diffusion_wavenet_bt_train.py + diff_wavenet_with_metavqvae_v4.yaml


Use text condition and T5 embedder
diffusion_wavenet_text_train.py + diffusion_wavenet_text.yaml + mmldm.models.diffusion.diffusion_wavenet_text +
mmldm.models.denoise_wavenet_v3 + mmldm.data.diffusion_wavenet_music_text_dataset + mmldm.diffusion_wavenet_text_logger


New VQVAE for joint music motion:
mm_vqvae_train.py + mm_vqvae.yaml + music_motion_dataset_v2 + mm_vqvae.py

version 2 of the new joint VQVAE. try to freeze all the music vqvae.
mm_vqvae_v2.yaml + mm_vqvae_v2.py

version 3 of the new joint VQVAE. Use meta vqvae's codebook.
mm_vqvae_v3.yaml + mm_vqvae_v3.py

version 4 of the new joint VQVAE. Use transformer as encoder and decoder
mm_vqvae_v4.yaml + mm_vqvae_v4.py

version 5 of the new joint VQVAE. Use freeze meta vqvae's codebook. But add another trainable layer
mm_vqvae_v5.yaml + mm_vqvae_v5.py

version 6 of the new joint VQVAE. Use freeze meta vqvae's codebook. But add some fusion layers for motion to extract information from music
mm_vqvae_v6.yaml + mm_vqvae_v6.py

version 7 of the new joint VQVAE. Improve the encoder and decoder in version 6.
mm_vqvae_v7.yaml + mm_vqvae_v7.py

version 8 of the new joint VQVAE. Change the input to 60 fps.
mm_vqvae_v8.yaml + mm_vqvae_v8.py + music_motion_dataset_v3.py

version 9 of the new joint VQVAE. Add loss on joint
mm_vqvae_v9.yaml + mm_vqvae_v9.py


music motion joint diffusion version 1: just concat two 128 dim latent vectors. Use text condition
mm_diff_train.py + mm_diff.yaml + mm_diff.py + music_motion_text_dataset.py

music motion joint diffusion version 2: change to pre-computed paired data
mm_diff_train.py + mm_diff_v2.yaml

music motion joint diffusion version 3: change to unet
mm_diff_train.py + mm_diff_v3.yaml

train our own joint mm encodec
music_vqvae_train.py + mm_encodec.yaml + mm_encodec.py

version 10 of the new joint VQVAE. Train all from scratch, using the same architecture as version 1
mm_vqvae_train.py + mm_vqvae_v10.yaml + mm_vqvae_v10.py

music motion joint diffusion version 4: change to transformer
mm_diff_train.py + mm_diff_v4.yaml

version 11 of the new joint VQVAE. Compress both music and motion to 128d, concat to 256d, then compress to 128d
mm_vqvae_train.py + mm_vqvae_v11.yaml + mm_vqvae_v11.py

version 12 of the new joint VQVAE. Compress both music and motion to 128d, and add them together.
mm_vqvae_train.py + mm_vqvae_v12.yaml + mm_vqvae_v12.py

try autoregressive transformer, only on music
mm_transformer.yaml + mm_transformer.py + music_text_dataset.py + mm_transformer_logger.py + mm_transformer_train.py

version 13 of the new joint VQVAE. Use linear layer to combine two 128d tensors   ----- not good. Motion loss too high
mm_vqvae_train.py + mm_vqvae_v13.yaml + mm_vqvae_v13.py

autoregressive transformer v2, change to predict 4 code each iteration, add condition dropout and classifier-free guidance
mm_transformer_v2.yaml + mm_transformer_v2.py

version 14 of the new joint VQVAE. Use v11 model, but motion dim to 128d. So the latent should be 256d.
motion recon loss: 0.19566193222999573, music recon loss: 0.011658965609967709, ref music loss: 0.00947776809334755

autoregressive transformer v3, try parallel generation. Use 4 codebooks and 4 prediction head

autoregressive transformer v4, try delayed pattern.

autoregressive transformer v5. Add weight init.

autoregressive transformer v6. Made some modification to mimic musicgen. And scale up!

version 15 of the new joint VQVAE. Add motion recon loss into balancer

version 14.2 of new joint vqvae. increase the motion weight from 50 to 100

version 16 of the new vqvae. Remove pre-post quant conv


mm transformer v7, use two separate vqvae
mm_transformer_v7_train.py + mm_transformer_v7.yaml + mm_transformer_v7.py

version 17 of the new vqvae. Encode the motion into 10fps 128 * (10 T) latent. Not using music code for auxiliary. Use only music codebook, and motion encoder decoder.

version 18 of the new vqvae. Encode the motion into 25 fps. Not using music code for auxiliary. Use only music codebook, and motion encoder decoder.


mm transformer v8. Use new data loader with more diverse music-motion pairs

mm transformer v9. Include a text decoder for music captioning

mm transformer v10. Use different empty token for music and motion; change attention mask in inference a bit (but I think it makes no difference); Use different prediction head in transformer for music and motion; change to sin encoding

mm vqvae v19. Change to 512 x 512 codebook. Use 25 hz encoding. Use the same network architecture, loss functions and scheduler as t2m-gpt

mm transformer v11. Add different weight on music and motion.

mm transformer v12. Use newly paired data. With text description of motion.

mm vqvae v20. don't use interpolation. Just 20hz. To see whether is the linear interpolation that make the network hard to train.
not good. loss: 0.25

mm transformer v13. Use pretrained MusicGen model for music motion joint generation.

mm transformer v14. Add a second stage to the model: text generation stage.
also a new training script joint_transformer_train_v2.py

mm transformer v15. use mixed tag caption and llama caption

mm transformer Ablation 1: remove modality encoding

mm transformer Ablation 2: remove motion linear classifier

mm transformer Ablation 3: use sequential generation

mm transformer Ablation 4: use interleaved generation

mm transformer Ablation 5: use separate motion vqvae code


